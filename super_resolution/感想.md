# 感想

## L1 loss / L2 loss

L2 loss converges much faster, but sensitive to the outliers.
Smooth L1 loss is an intermediate and good choice.
[L1 loss and L2 loss](https://zhuanlan.zhihu.com/p/48426076)

## multiscale / scale-invariant 

阅读论文 [Depth Map Prediction from a Single Image using a Multi-Scale Deep Network]

## side-input layer

可能在我们的问题里很重要
阅读论文 [Fully Convolutional Networks for Semantic Segmentation]

## contributions

- 结合 L1 L2 rank loss
  - L1 / L2 loss 是 pixel-wise, 是一种 local 信息
  - rank loss 施加了 global 信息

## Q

- 为何要结合 L1 L2 loss