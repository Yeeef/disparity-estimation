# 感想

## L1 loss / L2 loss

L2 loss converges much faster, but sensitive to the outliers.
Smooth L1 loss is an intermediate and good choice.
[L1 loss and L2 loss](https://zhuanlan.zhihu.com/p/48426076)

## multiscale / scale-invariant 

阅读论文 [Depth Map Prediction from a Single Image using a Multi-Scale Deep Network]
在原论文中, up-sampling 过程中, 会产生不同大小的 output-image, 对这些 image 进行上采样然后与 label 算出一个 Multi-scale loss

## side-input layer 

在我们的问题中, 利用 skip-connection 的思想来代替
阅读论文 [Fully Convolutional Networks for Semantic Segmentation]

## side-output layer

input to the multi-scale loss function

## ResNet

阅读 He 的原始论文，tensorpack 练习

## BN: batch normalization

阅读论文，Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift

加速训练，改善BP过程


## 网络结构

- downsampling: ResNet50
- upsampling:
  - 包括4个部分
  - 每个部分都有
    - un-pooling with kernel 2
    - convolutional layers with stride 1
    - side-in layer
    - side-output layer
  - 每个部分内部都有一个 skip connection
    - 将 un-pooling 的输出与最后一个卷积层输出相加，**可以改善 BP?**
  - 最后一层是一个 1 * 1 的卷积，把64 channel feature 变成输出的深度图 

## contributions

- 结合 L1 L2 rank loss
  - L1 / L2 loss 是 pixel-wise, 是一种 local 信息
  - rank loss 施加了 global 信息

## notations

- unpooling 可以通过 upsampling 来替代(近邻, 高斯等等, 这在 tf 里有封装好的函数)
- strided conv 可以改成下采样+卷积是

## Q

- 为何要结合 L1 L2 loss
  - L2 loss 对 outlier 过于敏感，可以对离群点施加很强的惩罚
  
- side-input layer 的具体作用

- 为何 skip-connection 可以改善 BP

- strided convolutional layer 和 convolutional layer 有什么区别？
上下两个问题其实是一个问题
- ResNet 论文的问题，
  - The element-wise addition is preformed on two feature maps
  - For both options, when the shortcuts go across feature maps of 2 sizes, they are performed with stride of 2
- concat 是怎么进行的？按照channel 叠吗？
  - 是的, 最后再用一个 conv layer, 让 channel 数恢复正常