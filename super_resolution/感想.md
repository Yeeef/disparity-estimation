# 感想

## L1 loss / L2 loss

L2 loss converges much faster, but sensitive to the outliers.
Smooth L1 loss is an intermediate and good choice.
[L1 loss and L2 loss](https://zhuanlan.zhihu.com/p/48426076)

## multiscale / scale-invariant 

阅读论文 [Depth Map Prediction from a Single Image using a Multi-Scale Deep Network]

## side-input layer 

可能在我们的问题里很重要
阅读论文 [Fully Convolutional Networks for Semantic Segmentation]

## side-output layer

input to the multi-scale loss function

## ResNet

阅读 He 的原始论文，tensorpack 练习

## BN: batch normalization

阅读论文，Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift

加速训练，改善BP过程


## 网络结构

- downsampling: ResNet50
- upsampling:
  - 包括4个部分
  - 每个部分都有
    - un-pooling with kernel 2
    - convolutional layers with stride 1
    - side-in layer
    - side-output layer
  - 每个部分内部都有一个 skip connection
    - 将 un-pooling 的输出与最后一个卷积层输出相加，**可以改善 BP?**
  - 最后一层是一个 1 * 1 的卷积，把64 channel feature 变成输出的深度图 

## contributions

- 结合 L1 L2 rank loss
  - L1 / L2 loss 是 pixel-wise, 是一种 local 信息
  - rank loss 施加了 global 信息

## Q

- 为何要结合 L1 L2 loss
  - L2 loss 对 outlier 过于敏感，可以对离群点施加很强的惩罚
  
- side-input layer 的具体作用

- 为何 skip-connection 可以改善 BP

- strided convolutional layer 和 convolutional layer 有什么区别？